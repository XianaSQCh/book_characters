{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5179b294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27270ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fce829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuralcoref\n",
    "import spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d795263",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('spacy version:', spacy.__version__)\n",
    "print('neuralcoref version:', neuralcoref.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c8034f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    try:\n",
    "        with io.open(path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            with io.open(path, 'r', encoding='latin-1') as f:\n",
    "                return f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            return \"Could not decode\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7869a07b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder = \"success\"\n",
    "\n",
    "# files sorted by number\n",
    "files_ordered = sorted(os.listdir(folder), key=lambda name:int(name.split('_')[0]))\n",
    "\n",
    "# an example file (1)\n",
    "file = files_ordered[2]\n",
    "# name file\n",
    "path = os.path.join(folder, file)\n",
    "\n",
    "if os.path.isfile(path):\n",
    "    content = read_file(path)\n",
    "    print(f\"Content of book ... {file}\\n\")\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6524a45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = content.split('\\n\\n')\n",
    "print(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c58d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs_new = []\n",
    "contents = []\n",
    "chapters = []\n",
    "old_idx = 0\n",
    "\n",
    "for paragraph in paragraphs:\n",
    "    # print(\"\\n Paragraph: \", paragraph)\n",
    "    \n",
    "    lines = paragraph.split('\\n')\n",
    "    # print(lines)\n",
    "    if re.match('[IVXLCDM\\d]+[\\.]*', lines[0].upper().strip()):\n",
    "        if len(contents) == 0:\n",
    "            contents = [\" \".join(line.lower().split()) for line in lines]\n",
    "            # print(\"contents\", contents)\n",
    "            \n",
    "            if len(contents) == 0:\n",
    "                print(\"Not found content\", id_file)\n",
    "                exit()\n",
    "        continue\n",
    "           \n",
    "    if paragraph.lower().strip() in contents:\n",
    "        if old_idx == 0:\n",
    "            old_idx = len(paragraphs_new)\n",
    "        else:\n",
    "            current_idx = len(paragraphs_new)\n",
    "            chapter = \"\\n\\n\".join(paragraphs_new[old_idx:current_idx])\n",
    "            chapters.append(chapter)\n",
    "            old_idx = current_idx\n",
    "        continue\n",
    "    \n",
    "    if not re.match('.*[\\w]+.*', paragraph):\n",
    "        continue\n",
    "    \n",
    "    paragraphs_new.append(paragraph)\n",
    " \n",
    "\n",
    "    if old_idx != 0:\n",
    "        chapter = \"\\n\\n\".join(paragraphs_new[old_idx:len(paragraphs_new)])\n",
    "        chapters.append(chapter)\n",
    "        \n",
    "print(len(paragraphs), len(paragraphs_new),\n",
    "      #len(chapters)\n",
    "     )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aa98cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92e0fe0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "geral_entities = []\n",
    "\n",
    "for i, paragraph in enumerate(paragraphs_new[0:15]):\n",
    "    paragraph = paragraph.replace('\\n', ' ')\n",
    "    paragraph = \" \".join(paragraph.split())\n",
    "    # print('\\n', paragraph)****\n",
    "    \n",
    "    doc = nlp(paragraph)\n",
    "    # print(doc)\n",
    "    \n",
    "    doc_coref = doc._.coref_resolved\n",
    "    doc_coref = nlp(doc_coref)\n",
    "    # print(doc_coref)\n",
    "    \n",
    "    entities = []\n",
    "    for ent in doc_coref.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            entities.append(ent.text)\n",
    "    # print(entities)\n",
    "    \n",
    "    if len(entities) > 0:\n",
    "        len_words = len(paragraph.split())\n",
    "        geral_entities.extend(entities)\n",
    "        # print('\\n', paragraph)\n",
    "        print(i, len_words, entities)\n",
    "    \n",
    "print(\"\\nGeral Entities:\\n\", set(geral_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7752294a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b58b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    # (0, 91, ['Young Hillocks', 'Hillocks', 'Drumsheugh']),\n",
    "    (1, 168, ['Keep', 'Dominie', 'Keep', 'Keep', 'Geordie Hoo', 'Keep']),\n",
    "    # (2, 160, ['Tonic Sol-fa']),\n",
    "    # (4, 188, ['Drumtochty', 'Dominie Jamieson']),\n",
    "    # Add more data\n",
    "]\n",
    "\n",
    "# Create a graph using NetworkX\n",
    "G = nx.Graph()\n",
    "\n",
    "# Iterate through data and extract character co-occurrences\n",
    "for _, _, characters in data:\n",
    "    for i, character in enumerate(characters):\n",
    "        for other_character in characters[i + 1:]:\n",
    "            if G.has_edge(character, other_character):\n",
    "                G[character][other_character]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(character, other_character, weight=1)\n",
    "\n",
    "# Visualization using NetworkX\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "edge_labels = {(u, v): f\"{d['weight']}\" for u, v, d in G.edges(data=True)}\n",
    "\n",
    "nx.draw(G, pos, with_labels=True, node_size=100, font_size=10, font_color='black', font_weight='bold')\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)\n",
    "\n",
    "plt.title(\"Character Co-occurrence Network\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb07943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Informações fornecidas\n",
    "edges = [\n",
    "    (\"Philip Horn\", \"William Cliff\"),\n",
    "    (\"Philip Horn\", \"Castor\"),\n",
    "    (\"Philip Horn\", \"Edna Markham\"),\n",
    "    (\"Philip Horn\", \"Ralph\"),\n",
    "    (\"William Cliff\", \"Castor\"),\n",
    "    (\"William Cliff\", \"Edna Markham\"),\n",
    "    (\"William Cliff\", \"Ralph\"),\n",
    "    (\"William Cliff\", \"Ralph Markham\"),\n",
    "    (\"Castor\", \"Edna Markham\"),\n",
    "    (\"Edna Markham\", \"Ralph\"),\n",
    "    (\"Ralph\", \"Ralph Markham\"),\n",
    "    (\"Ralph\", \"Cliff\"),\n",
    "    (\"Ralph\", \"Markham\"),\n",
    "    (\"Ralph\", \"Maka\"),\n",
    "    (\"Ralph Markham\", \"Cliff\"),\n",
    "    (\"Cliff\", \"Markham\"),\n",
    "    (\"Captain Horn\", \"Horn\"),\n",
    "    (\"Captain Horn\", \"Rynders\"),\n",
    "    (\"Rynders\", \"Maka\")\n",
    "]\n",
    "\n",
    "# Criação do grafo\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Visualização do grafo\n",
    "pos = nx.spring_layout(G)\n",
    "plt.figure(figsize=(10, 8))\n",
    "nx.draw(G, pos, with_labels=True,\n",
    "        node_color=\"skyblue\", node_size=300, font_size=9, font_weight='bold', edge_color='gray', width=1)\n",
    "plt.title(\"Rede de Personagens do Livro\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc76c3d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b73f93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9578f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (new_env)",
   "language": "python",
   "name": "new_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
